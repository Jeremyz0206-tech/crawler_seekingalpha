
# ğŸ•·ï¸ ä½¿ç”¨ Playwright çˆ¬å– SeekingAlpha æ–°é—»æ ‡é¢˜ï¼ˆmacOS æ•™ç¨‹ï¼‰

## ğŸ“Œ ç›®å½•

1. [å®‰è£…ç¯å¢ƒ](#1-å®‰è£…ç¯å¢ƒ)
2. [ç¤ºä¾‹ç›®æ ‡](#2-ç¤ºä¾‹ç›®æ ‡)
3. [å®Œæ•´è„šæœ¬ï¼ˆé€‚é… Jupyter å’Œ .py æ–‡ä»¶ï¼‰](#3-å®Œæ•´è„šæœ¬é€‚é…-jupyter-å’Œ-py-æ–‡ä»¶)
4. [è¿è¡Œæ•ˆæœ](#4-è¿è¡Œæ•ˆæœ)
5. [å¯é€‰è¿›é˜¶ï¼šæ»šåŠ¨åŠ è½½ä¸æ‘˜è¦æŠ“å–](#5-å¯é€‰è¿›é˜¶æ»šåŠ¨åŠ è½½ä¸æ‘˜è¦æŠ“å–)

---

## âœ… 1. å®‰è£…ç¯å¢ƒ

åªéœ€ä¸€æ¬¡æ€§æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
pip install playwright nest_asyncio pandas tqdm
playwright install
```

---

## ğŸ“ˆ 2. ç¤ºä¾‹ç›®æ ‡

çˆ¬å–ä»¥ä¸‹ç½‘å€ä¸­æœ€æ–°çš„æ–°é—»æ ‡é¢˜å’Œé“¾æ¥ï¼ˆå¯æ¢è‚¡ç¥¨ï¼‰ï¼š

```
https://seekingalpha.com/symbol/AAPL/news
https://seekingalpha.com/symbol/TSLA/news
```

---

## ğŸ’» 3. å®Œæ•´è„šæœ¬ï¼ˆé€‚é… Jupyter å’Œ .py æ–‡ä»¶ï¼‰

```import pandas as pd
from tqdm.asyncio import tqdm
import asyncio
from playwright.async_api import async_playwright
import nest_asyncio
import os

# Jupyter Notebook è¡¥ä¸ï¼šè§£å†³ asyncio loop å†²çª
nest_asyncio.apply()

# è·å–ç”¨æˆ·æ¡Œé¢è·¯å¾„
desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")

# è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
symbols = ["AAPL", "TSLA", "NVDA"]

async def scrape_symbol(symbol):
    print(f"\nğŸ” Scraping {symbol}...")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context()
        page = await context.new_page()

        url = f"https://seekingalpha.com/symbol/{symbol}/news"
        await page.goto(url, timeout=60000)

        try:
            await page.wait_for_selector('a[data-test-id="post-list-item-title"]', timeout=15000)
        except:
            print(f"âŒ Failed to load articles for {symbol}")
            await browser.close()
            return

        article_elements = await page.query_selector_all('a[data-test-id="post-list-item-title"]')
        data = []
        for tag in article_elements:
            title = await tag.inner_text()
            href = await tag.get_attribute("href")
            full_url = "https://seekingalpha.com" + href
            data.append({"title": title.strip(), "url": full_url})

        await browser.close()

        if data:
            df = pd.DataFrame(data)
            filename = os.path.join(desktop_path, f"{symbol.lower()}_news_playwright.csv")
            df.to_csv(filename, index=False, encoding="utf-8-sig")
            print(f"âœ… Saved {len(df)} articles to {filename}")
        else:
            print(f"âš ï¸ No news found for {symbol}")

# å¯åŠ¨ä»»åŠ¡
await asyncio.gather(*(scrape_symbol(symbol) for symbol in symbols))

```

---

## ğŸ“‚ 4. è¿è¡Œæ•ˆæœ

- è¾“å‡ºæ–‡ä»¶ï¼š
  - `aapl_news_playwright.csv`
  - `tsla_news_playwright.csv`
- æ¯ä¸ª CSV å«æ ‡é¢˜å’Œé“¾æ¥ï¼š

| title                              | url                                                |
|------------------------------------|-----------------------------------------------------|
| Apple stock rallies on Q2 earnings | https://seekingalpha.com/news/123456-aapl-earnings |

---

## ğŸš€ 5. å¯é€‰è¿›é˜¶ï¼šæ»šåŠ¨åŠ è½½ä¸æ‘˜è¦æŠ“å–

ä½ å¯ä»¥æ‰©å±•æ­¤è„šæœ¬å®ç°ï¼š

- è‡ªåŠ¨å‘ä¸‹æ»šåŠ¨åŠ è½½æ›´å¤šæ–°é—»
- æŠ“å–æ‘˜è¦ã€å‘å¸ƒæ—¶é—´ã€ä½œè€…
- åˆå¹¶å¤šè‚¡ç¥¨ç»“æœä¸ºä¸€ä¸ª DataFrame è¾“å‡º Excel

---

ğŸ§  æœ¬é¡¹ç›®é€‚åˆç”¨äºæ–°é—»çˆ¬è™«ç»ƒä¹ ã€æŠ•èµ„ç ”ç©¶ã€æˆ–æ„å»º RSS æ›¿ä»£æ–¹æ¡ˆã€‚
